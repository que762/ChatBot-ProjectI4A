{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA version used is : \" + torch.version.cuda)\n",
    "print(\"Total GPU memory is : \" + str(torch.cuda.get_device_properties(0).total_memory / 1024**3) + \" GB\")\n",
    "print(\"Total GPU memory used is : \" + str(torch.cuda.memory_allocated(0) / 1024**3) + \" GB\")\n",
    "print(\"Total GPU memory cached is : \" + str(torch.cuda.memory_reserved(0) / 1024**3) + \" GB\")\n",
    "print(\"Total GPU memory free is : \" + str((torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1024**3) + \" GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "access_token = \"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\"\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "cache_dir = (\"./models\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Load the model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    cache_dir=cache_dir, \n",
    "    token=access_token, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    cache_dir=cache_dir, \n",
    "    token=access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explique qui est Thomas Pesquet et ce qu'il a fait dans la santé?\n",
      "\n",
      "Thomas PesQUET est un médecin français spécialisé en médecine chinoise. Il est connu pour son travail sur la prévention des maladies neurologiques, et plus particulièrement pour sa contribution à la recherche sur les maladies du cerveau et la maladie de Alzheimer.\n",
      "\n",
      "Ses recherches ont porté sur des sujets tels que les effets des inondations cérébrales, les troubles neurodépressifs, le cholestérol, l'inflammation neurologique et les mutations génétiques. Son travail a également contribué à définir des diagnostic et des traitements pour les pathologies neuroméraires.\n"
     ]
    }
   ],
   "source": [
    "input_text = r\"\"\"Explique qui est Thomas Pesquet et ce qu'il a fait\"\"\"\n",
    "\n",
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=1024 + len(input_text),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=2,  # Prevent repetition of 2-grams\n",
    "        temperature=0.7,         # Control the creativity of the model\n",
    "        top_k=50,                # Top-k sampling\n",
    "        top_p=0.9               # Nucleus sampling\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b54d514159b45b3851c782dfa502f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "access_token = \"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\"\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "cache_dir = (\"./models\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Load the model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    cache_dir=cache_dir, \n",
    "    token=access_token, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    cache_dir=cache_dir, \n",
    "    token=access_token\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-projet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
